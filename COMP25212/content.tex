% Set the author and title of the compiled pdf
\hypersetup{
	pdftitle = {\Title},
	pdfauthor = {\Author}
}

\section{Introduction}

Performance is always an attribute in high demand in computer systems. Even
though processors have become much more powerful over the last half century,
there's still loads of stuff that we cannot do with current technology, such as
synthesising HD video in realtime, or computing realistic game physics.

Since 2004/5, companies haven't been able to increase the speed of
microprocessors at such a rapid rate due to physical limits, such as power
dissipation and device variability. Our devices are still getting faster, but
now architecture and the design of systems play a larger role in making stuff
run faster. An example of this include making computation more parallel.

\section{Caches}

Not all technology has improved at the same relative speed. CPU's have become
over three orders of magnitude faster over the past thirty years, but memory has
increased by only one order of magnitude. This is problematic, since it means
that we need to reconcile this gap in order to achieve efficient computation.

Processor caching is used to let the processor do useful computation while
it's also waiting on the memory. Modern processors couldn't perform anywhere
near how fast they do now without equally modern caching techniques, since the
imbalance between the CPU and main memory is so high.

Caches (in general) provide a limited, but very fast local space for the CPU to
use. They are used in lots of places all over computer science, including web
browsers, mobile phone UI's etc. Likewise, a processor cache is a temporary
store for frequently used memory locations.

The principle of locality is what makes caches work for processors, which is
that the CPU will only use a small subset of memory over a short period of time.
If this subset of memory can be loaded into the cache, then the computation can
be sped up significantly.

Every `cache miss' takes \textit{at least} sixty times longer to execute than a
`cache hit' will (that's assuming there are no page faults etc). Circuit
capacitance is the thing that makes electronic devices slow, and larger
components have a larger capacitance, henceforth large memories are slow.
Dynamic memories (DRAM) store data using capacitance, and are therefore slower
than static memories (SRAM) that work using bistable circuits.

Even the wires between the processor and the memory have a significant
capacitance. Driving signals between chips needs specialised high power
interface circuits. An ideal situation would be to have everything on a single
chip, however current manufacturing limitations prevent this; maybe one day we
will be able to do this.

% TODO: Memory heirachy diagram

\subsection{Why are caches expensive?}

L1, L2 and (usually) L3 caches are SRAM instead of DRAM (which is what main
memory is made from).

SRAM needs six transistors per bit, DRAM needs one.

SRAM is henceforth physically larger, taking up more space on the chip, which is
expensive, since real estate costs money.


\subsection{L1 Cache}

The L1 cache is the first level of caching between the processor and the main
memory. The L1 cache is around 32kb, which is very small in comparison to the
size of the main memory, but this is driven out of necessity, since the cache
needs to be small to be fast. The cache must be able to hold any arbitrary
location in the main memory (since we don't know in advance what the CPU will
want), and henceforth requires specialised structures to implement this.


\subsection{Types of cache}

The CPU will give the cache a full address of a word in memory that it wants to
read. The cache will contain a small selection of values from memory that it has
locally, but will ask the main memory for values that it does not have. This is
called a \textit{cache miss} and is expensive in comparison to a cache hit.

\subsubsection{Fully associative}

A \textbf{Fully Associative} cache is one where the cache is small (around
32,000 values), but stores both addresses and their corresponding data. The
hardware compares the input address with all of the stored addresses (it does
this in parallel). If the address is found, then the value is returned with no
need to ask the RAM (cache hit), if the value isn't found, then a cache miss
occurs, and the request must go to the main memory.

Caches rely on locality in order to function effectively. There are two types of
locality; temporal locality, which is the principle that if you use an address
once, you may use it again soon (e.g. loops), and spatial locality, where if you
use an address once, you are also likely to use addresses nearby (e.g. arrays).

\marginpar{Spatial locality is exploited better by having a bigger data area in
the cache (returning say 512 bits for every address instead of just one word)}

The cache hit rate is the ratio of cache hits to misses. We usually need a hit
rate of around $98\%$ to hide the speed of memory from the CPU. Instruction hit
rates are usually better than data hit rates (although they are in the same
cache remember). The reason for this is that instructions are accessed in a more
regular pattern, incrementing by one word every time, or looping around etc
(have higher locality).

When we do a cache miss (on a read), we should add the missed value to the
cache. In order to do this, we need a cache replacement policy to find room in
the cache to put the new value. Common cache replacement algorithms include:

\marginpar{See my \texttt{COMP25112} notes for in depth stuff about LRU, Round
Robin and more cache replacement algorithms (though they'll be under
\textit{page} replacement algorithms or scheduling algorithms there).}

\begin{description}
	\item \textbf{Last Recently Used} (LRU) - slow, good for hit rates
	\item \textbf{Round Robin} - not as good, easier to implement
	\item \textbf{Random} - Easy to implement, works better than expected.
\end{description}

Memory writes are more complicated than reads. If we've already got the value in
the cache, then we change the value in the cache. We can use three write
strategies for cache writes (on hits) to ensure that the changes are propagated
to memory:

\begin{itemize}
	\item Write through (slow)
	\item Write through + buffer (faster, slow when heavily used)
	\item Copy back on cache replacement.
\end{itemize}

If however, we have a miss, then:

\begin{itemize}
	\item We can find a location in the cache and write to that, then rely on
	copy back later, or write back straight away.
	\item We can skip the cache, and write directly to RAM. Subsequent reads to
  this memory location will	put it in the cache again (this is good if you're
  initialising datastructures	with zeroes).
\end{itemize}

\marginpar{A cache is coherent with memory if the values stored in the cache
are the same as those in memory. This is always desirable (since otherwise
you could have `old' data somewhere in the system), but sometimes hard to
achieve.}

The fastest strategy of these two is write allocate or copy back (the first
one). However, the main memory and the cache aren't coherent, which can be a
problem for stuff like multiprocessors, autonomous IO devices etc. This can
cause problems, which we will deal with later.

Each cache line is at least half address and half data, but often, we store more
data per address, so will have 64 bytes of data per 32 bit address.

A fully associative cache is ideal, but this is expensive (in terms of silicon
and power).

\subsubsection{Directly mapped}

We can use standard RAM to create a directly mapped cache, which mimics the
functionality of an ideal cache. Usually, this uses static RAM, which is more
expensive than dynamic RAM, but is faster. The address is divided into two
parts, the tag and the index. The tag is the higher order bits of the address
and the index is the lower order bits. The index is used to address the slot in
the cache, and is made of the number of bits required to do this (so 15 bits for
a $32\si{\kilo\bit}$ cache for example). The tag is used to check that the data
stored for this index value is the correct address, this is probably described
better by looking at Figure~\ref{fig:direct-mapped-cache}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{images/direct-mapped-cache}
  \caption{A direct mapped cache has one slot for multiple different memory
  addresses, the index determines what slot the value goes in, and the tag
  checks that the correct value is being read.}
  \label{fig:direct-mapped-cache}
\end{figure}

A direct mapped cache is really just a hash table implemented in hardware, but
the collision policy is eviction (so if two addresses have the same index, then
the one currently in the cache will be evicted to make room for the new one).

This is a very cheap replacement policy (since no algorithm is implemented to
decide what cache line to evict next), and it exploits spatial locality to
minimise displacing recently used data. However, it does have a lower hit rate
than more complex algorithms since the replacement strategy is so inflexible.

\subsubsection{Set associative}

Set associative caches are a compromise. They comprise of a number of directly
mapped caches operating in parallel. If one matches, we have a hit and select
the appropriate data. This is good because we can have more flexible cache
replacement strategies. In a 4 way set associative cache, we could choose any
one of the four caches for example. The hit rate of set associative caches
improves with the number of caches, but increasing the number increase the cost.

\section{Practical caches}

\subsection{Cache control bits}

When the system is started, the cache is empty. We need a bit for each cache
entry to indicate that the data is meaningful (i.e. it isn't just an
uninitialised zero or something), which we do by having a valid bit. We also
need a dirty bit if we're using the `write back' caching strategy (see above),
rather than the `write through' strategy.

\subsection{Exploiting spatial locality}

In order to exploit spatial locality, we need to have a wider `cache line',
where each entry will give you more data than just one word. Each entry tag
could correspond to two, four, eight etc words. The principle of spatial
locality says that if we access memory at one location, we'll probably want to
access nearby locations in the near future too.

\marginpar{The 16/32 byte figure might be outdated now!}

The lowest bits are used to select the word in the cache line. Most cache lines
are 16 or 32 bytes, which is 4 or 8 32bit words. The data is transferred from
RAM in bursts equal to the width of the line size, using specialised memory
access modes.

%TODO: Explain about more cache line = less number of tags? Is that even right?

The line size is important, since we want to have a line size of multiple words
to exploit spatial locality, but if the line is too big, then parts of it will
never be used. The number of cache misses decreases as you increase the cache
line size, until one point, where the line size will be too long to use all the
words, and then the number of misses will increase.

\subsection{Separate instruction and data caches}

Studies have shown that programs typically do one access to memory for data per
three access for an instruction. The data accesses are usually (except in the
rare case of self modifying programs) in separate address ranges, and therefore
these different access patterns can be exploited by having separate instruction
and data caches. This is called a Harvard architecture.

To cater for this, the L1 CPU cache is usually split up into the L1I cache for 
instructions and the L1D cache for data.

\subsection{Multi level caches}

As chips get bigger, in theory, we should build bigger caches to perform better.
However, big caches are slow, and the L1 cache needs to run at processor speed.
We can instead put another cache between the RAM and the L1 cache, and keep the
L1 cache the same size.

The L2 cache is typically around sixteen times bigger than the L1 cache, but
also four times slower. It's still ten times faster than RAM though. The L1D and
L1I caches both share the L2 cache.

If a chip has an L3 cache, then it is usually quite large (maybe around 8Mb),
but its performance is only about twice as good as that of RAM.

%TODO: Cache example from slide 16

\subsection{Cache misses}

There are three types of cache misses, called the three C's

\begin{description}
  \item \textbf{Compulsory misses}\\ 
    When we first start the computer, the cache is empty, so until the cache is
    populated, we're going to have a lot of misses.
  \item \textbf{Capacity misses}\\
   Since the cache is limited in size, we can't contain all of the pages for a
   program, so some misses will occur.
  \item \textbf{Conflict misses}\\
    In a direct mapped or set associative cache, there is competition between
    memory locations for places in the cache. If the cache was fully
    associative, then misses due to this wouldn't occur.
\end{description}

We can calculate the impact of cache misses, predicting on average, how long the
CPU will wait for a reply from memory. If the L1 cache has a hit rate of $98\%$
and takes one cycle to return a value, the L2 cache has a hit rate of $90\%$
and takes four cycles to return, the L3 cache has a hit rate of $70\%$ and takes
ten cycles, while the main memory takes 100 cycles, what is the average access 
time?

First, we can calculate how many accesses (as a percentage) each level will get:

\begin{tabular}{l >{$}l<{$}}
  L1 & 98\%\\
  L2 & ((100-98)*0.9) = 1.8\%\\
  L3 & ((2-1.8)*0.7) = 0.14\%\\
  Main Memory & ((0.2-0.14)*1) = 0.06\%
\end{tabular}

We can then use those as decimals to calculate how many clock cycles the CPU
will have to wait on average:

\[
  \begin{split}
    \text{Average} &= 0.98(1) + 0.018(1 + 4) + 0.0014(1 + 4 + 10) + 0.0006(1+4+10+100)\\
                   &= 0.98 + 0.09 + 0.021 + 0.069\\
                   &= 1.16
  \end{split}
\]

\subsection{More cache performance}

%TODO: What happens when IO is memory mapped, does this go through the cache? :o

In order to fill a cache from empty, it takes $\frac{\text{Cache
size}}{\text{Line size}}$ memory accesses. If we multiply this by the time it
takes for a single memory access (say $10\si{\micro\second}$), then we can work
out how long it will take to fill the cache (assuming each access is to a unique
memory address). We can derive how many CPU cycles this takes from this.

\subsection{Cache consistency}

We need to make sure that the values stored in the CPU cache are consistent with
those in main memory. There are situations when they can disagree, for example
if IO reads or writes directly to memory (perhaps using DMA), then that value
could be different from whatever is in the cache.

There are a number of solutions to this:

\begin{description}
  \item \textbf{Non-cacheable}\\
    One solution is to make areas of memory that IO can access non-cachable, or
    clear the cache before and after the IO takes place.
  \item \textbf{IO use data cache}\\
    Another is to have the IO go directly through the CPU's L1D (data) cache
    before accessing memory, but this tends to slow down the cache (which is
    obviously bad for the speed of the system, but we'll look why in
    Section~\ref{pipelines}).
  \item \textbf{Snoop on IO activity}\\
    We could have hardware logic in the cache that will look at the reads and
    writes to memory from IO and make sure the cache is consistent with memory
    for those addresses. This will be covered more in Section~\ref{snooping}.
\end{description}

\subsection{Virtual Addresses}

Since the CPU deals with virtual addresses when accessing memory, and uses a
Translation Lookaside Buffer to derive the correct physical address. However,
which address does the cache store? Does it sit before the TLB, or after it
between the CPU and memory?

If we make addresses go through the TLB before they reach the cache, then this
is slow, since they must pass through extra logic etc before hitting the cache.
However, if we make the cache store virtual addresses, and have the TLB sit
inbetween the cache and memory, this makes snooping hard to implement along with
other functional difficulties. This is illustrated in Figure~\ref{tlb-pos}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.45\textwidth]{images/tlb-pos-before}
  \includegraphics[width=0.45\textwidth]{images/tlb-pos-after}
  \caption{Two possible placements of the Translation Lookaside Buffer}
  \label{tlb-pos}
\end{figure}

The answer is to have the TLB operate in parallel to the cache. Since address
translation only affects the high order bits of the cache (the low order bits
are the offset which remains the same). The cache index is selected from the low
order offset bits, and only the tag is changed by address translation. This is 
shown in Figure~\ref{tlb-parallel}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{images/tlb-parallel}
  \caption{The TLB operating in parallel with the CPU cache}
  \label{tlb-parallel}
\end{figure}

\section{Pipelines}
\label{pipelines}

The fetch execution cycle is very repetitive, if we can optimise it, then we can
potentially improve the performance of the system a lot! In very simple systems,
each fetch and execute would take one clock cycle; in detail, it'd look like
this:

\begin{description}
  \item \textbf{Fetch}
    \begin{description}
      \item \textbf{IF} - Fetch instruction from memory
      \item \textbf{ID} - Decode instruction; select registers
    \end{description}
  \item \textbf{Execute}
    \begin{description}
      \item \textbf{EX} - Perform an operation or calculate an address
      \item \textbf{MEM} - Access an operand in memory
      \item \textbf{WB} - Write to registers
    \end{description}
\end{description}

If all that takes one clock cycle, each stage will only be active for about
$\frac{1}{5}$ of a clock cycle, or in other words, each CPU component spends
$80\%$ of its time doing nothing!

If we can get all of those components of the CPU working at the same time, then
we can speed up the clock speed by five times! In order to do this, we can use a
pipeline, with buffers that are flushed every clock cycle inbetween each stage
of the pipeline, as shown in Figure~\ref{basic-pipeline}.

\marginpar{Although we have divided our processor into five stages, you can
split it up into more or less than that (the most simple of which is just to
divide into fetch and execute as we did in \texttt{COMP12111} in the first
year. Modern processors use a lot of stages, maybe around thirty.)}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/basic-pipeline}
  \caption{Using buffers between the different stages of the pipeline}
  \label{basic-pipeline}
\end{figure}

Now, we can still only execute one instruction per cycle, but we can also
increase our clock speed by five times, since each stage of the pipeline is
isolated by a buffer from the next, and only does $\frac{1}{5}$ of the work of a
`normal' CPU cycle.

\subsection{Control Transfer Problem}

Using a pipeline is fab if your instructions occur solely in a serial manner,
however, what if your program branches? The processor will only know that a
branch is happening at the \texttt{ID} stage of the pipeline, by which time
we've already fetched the next instruction!

If we come across a branch at the \texttt{ID} stage, then the fetched
instruction at the \texttt{IF} stage will have to be ignored all the way down
the pipeline, so we would waste one clock cycle (or more specifically, we would
waste $\frac{1}{5}$ of the work of five clock cycles!). The ignored instruction
is said to be a \textit{bubble} in the pipeline.

So far, we've assumed that our branch instruction has relied on no conditional
flags. If it did however, we would need to wait until the \texttt{EX} stage of
the pipeline before we knew what the outcome of the conditional evaluation would
be. This has the potential to create two bubbles, since if the branch was to
occur, the instructions at both the \texttt{IF} and \texttt{ID} stages of the
pipeline would have to be ignored.

\marginpar{Other control hazards are possible, it's not just branching that's
the issue. Conditional instructions (e.g. \texttt{MOVGE}) can also be cause
control hazards.}

These bubbles are called \textbf{control hazards}, and they occur when it takes
one or more pipeline stages to detect a branch. Longer pipelines are more likely
to suffer from control hazards more, since more of their pipeline will have been
processed by the time an instruction is detected to be a branch.

\subsubsection{Branch Prediction}

The main technique used to mitigate control hazards is \textbf{branch
prediction}. If we can remember what address a branch directed us to fetch next
from what it did when we executed that branch previously, then we could
pre-emptively load that instruction in the \texttt{IF} stage instead of fetching
the instruction at the PC.

In order to do this, we use a \textbf{branch target buffer}. This maps the
virtual address of one branch instruction onto the virtual address of the
instruction that is branched to, for example:

\begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Branch instruction address} & \textbf{Next instruction address} \\ \hline
    \texttt{0xd4f30d2C} & \texttt{0xd4f30d60} \\ \hline
    \texttt{0xd4f30d0f} & \texttt{0xd4f30ddd} \\ \hline
    \multicolumn{1}{c}{\vdots} & \multicolumn{1}{c}{\vdots\vspace{0.2em}}\\ \hline
    \texttt{0xd4f30c4f} & \texttt{0xd4f30d6c} \\ \hline
  \end{tabular}
  \caption{An abstraction of what the datastructure inside a Branch Target
  Buffer could be like}
\end{table}

Now, we will always get unconditional branches right (after we've done it for
the first time), and we will get branches that are part of loops (such as a
\texttt{for} or a \texttt{while} loop) right most of the time. If we predict a
branch incorrectly, we just get a bubble like we would if we had no branch
prediction, since the steps after the incorrectly predicted branch will just
be aborted. For efficiency, branch prediction is a win-win situation.

Branch prediction is easy to understand, but implementing it is expensive. In
practice branch predictors use the history of each branch (maybe taking the mode
of the last five branches), and the context of the branch (i.e. how did we get
to this point) in order to make a more accurate prediction.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{images/branch-target-buffer}
  \caption{A BTB (Branch Target Buffer) in action}
  % TODO: Work out what this diagram is *actually* showing...
  \label{branch-target-buffer}
\end{figure}

\subsection{Data Hazards}

If we're using a pipeline to process instructions, then we have multiple
instructions at various stages of execution at the same time. This can cause
problems, if multiple instructions that are being executed in parallel operate
on the same resources. For example, if we execute the ARM Assembly code:

\begin{verbatim}
  ADD   R1, R2, R3
  MUL   R0, R1, R1
\end{verbatim}

\marginpar{This problem is called a `data not ready' issue.}

\marginpar{It might take two cycles for a value to be written into a register,
one for \texttt{EX} to complete and push the value into the register bank, and
another for the value to be written into the register.}

Here, the value in \texttt{R0} depends on that of \texttt{R1}. This is a
problem, because we only know the value of \texttt{R1} once the \texttt{ADD}
instruction has finished the \texttt{EX} stage, and the \texttt{MUL} instruction
will get the values from the registers while this is happening in it's
\texttt{ID} stage, henceforth we won't read the correct value of \texttt{R1}.

Two easy solutions to this problem are to:

\begin{itemize}
  \item Detect inter-instruction dependencies in hardware and withhold
  instructions in the decode stage until the data is ready. This creates the
  bubbles that we've worked so hard to avoid with branch prediction though!

  \item Have a compiler detect the dependencies, and have it re-order
  instructions to eliminate them. This is hard to do though, and often results
  in compilers inserting \texttt{NOP} (no operation) instructions that do
  nothing. These act like bubbles anyway, so we don't gain anything here either.
\end{itemize}

Since these are unsatisfactory, we could add extra paths to the pipeline between
the ALU output and the ALU input. They could be activated if there is an
interdependency so that the incorrect result of the \texttt{ID} stage could be
modified before being processed by the ALU, as shown in
Figure~\ref{extra-paths}.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{images/extra-paths}
  \caption{Here, we insert extra paths into the processor so that it can
  mitigate the \texttt{ADD}/\texttt{MUL} dependency.}
  \label{extra-paths}
\end{figure}

What if there is an interdependency with an instruction that might take a while
to execute, such as a memory read (\texttt{LDR})? If we have a program that
does:

\begin{verbatim}
  LDR   R1, [R2, R3]
  MUL   R0, R1, R1
\end{verbatim}

Our path would have to look like in Figure~\ref{more-extra-paths}.

\marginpar{Adding extra paths to the architecture to pass updated register
values back to previous stages of the pipeline is called \textbf{forwarding}.}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{images/more-extra-paths}
  \caption{In order to cater for the worst case, where we're waiting on a memory
  read, we need even more extra paths.}
  \label{more-extra-paths}
\end{figure}

We want longer pipelines, since we want each stage of the pipeline to do as
little work as possible, taking as little time as possible, so that we can
increase the clock speed. However, as we do that, we will come across more
hazards, and we'll need to put more control paths in to mitigate them.
Eventually, there will come a point where increasing the length of the pipeline
will result in negative returns.

\subsection{Instruction Level Parallelism}

If there are instructions that do not depend on each other at all, for example:

\begin{verbatim}
  ADD   R0, R2, R3
  SUB   R1, R4, R5
\end{verbatim}

Then we could run these instructions at the same time, and not experience any
side effects. We may run into problems, if there are instructions that depend on
these two instructions having completed before they themselves execute, for
example:

\begin{verbatim}
  ADD   R0, R2, R3
  SUB   R1, R4, R5
  MUL   R0, R0, R1
  STR   R0, x
\end{verbatim}

We can draw a data flow graph to visualise these dependencies, and easily see
which instructions can be run in parallel, as in Figure~\ref{data-flow}.

\begin{wrapfigure}{o}{0.4\textwidth}
  \centering
  \vspace{-2em}
  \includegraphics[width=0.4\textwidth]{diagrams/data-flow.pdf}
  \vspace{-2em}
  \caption{A sample data flow graph.}
  \vspace{-2em}
  \label{data-flow}
\end{wrapfigure}

Though this is a simple example, analysis has shown that it is not uncommon for
real programs to have up to four instructions that can be parallelised at some
point in the program's running time. Obviously, the amount of times when two or
three instructions could be parallelised will be greater than that of four,
which increases the argument for ILP.

In order to exploit this parallelism, we can:

\begin{itemize}
  \item Fetch multiple (in our case two for simplicity) instructions per cycle.
  \item Have multiple ALU's to execute instructions in parallel.
  \item Have common registers and caches, since the instructions are operating
  on the same data.
\end{itemize}

This could require an architecture looking like that in
Figure~\ref{superscalar}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{images/superscalar}
  \caption{A simple superscalar (implementing ILP) architecture.}
  \label{superscalar}
\end{figure}

\marginpar{Note, these multiple reads and writes will be occurring in different
parts of the register bank and cache, since the parallel instructions are (by
nature) independent of each other.}

If we are able to run multiple instructions at the same time, then our registers
and caches are going to have (in this case, which is a low level of parallelism)
roughly twice the load that they had previously. In order to allow them to handle
this extra load, we can make them \textbf{dual ported}, which means that the
access circuitry is duplicated so multiple reads and writes can happen
simultaneously.

In order to implement ILP, we also need a `dispatch unit' in hardware which is
part of the fetch stage (\texttt{IF}). This will fetch multiple instructions if
they are independent and can be executed in parallel.

\subsubsection{Out of Order Execution}

In order to get the maximum number of ILP compatible instruction sequences in a
program, the compiler may be able to re-order instructions so that they have a
reduced number of interdependencies. One technique that is based on this is the
Very Long Instruction Word, where each word will be longer than a normal word
(maybe 48, 64 or more bits), and will contain more than one instruction. Since
the compiler decides what instructions to execute in parallel by putting them in
a VLIW, the processor doesn't need to do so at runtime, therefore the complexity
of the instruction scheduling is taken away from the CPU and its complexity can
be reduced.

Having the compiler re-order instructions means that it will sometimes add
\texttt{NOP}'s into the code, which can increase the binary size, and bloat the
code. The alternative it to rely on expensive hardware to detect out of order
opportunities at runtime.

To implement an out of order processor, you need to have a buffer that
instructions are fetched into, a scheduler to choose which (non-conflicting)
instructions to execute at what times, and a cache to store memory and register
accesses until all the instructions have finished so that the application can
execute normally as though all instructions executed in serial.
Figure~\ref{out-of-order} shows how this could all be implemented.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/out-of-order}
  \caption{An example of how an out-of-order processor could be implemented.}
  \label{out-of-order}
\end{figure}

\subsubsection{Programmer assisted ILP}

Some processors now support SIMD (Single Instruction Multiple Data)
instructions, that allow one instruction to be executed on multiple
registers/memory locations etc. This is very useful when doing mathematical
computation with stuff like vectors and matrices, since it requires a lot of
repetitive operations that lend themselves easily to parallelism.

One example of this is the \texttt{UADD8} ARM instruction. It adds two registers
together into a third register like a normal add, but it does this four times;
once for each eight bit chunk in the registers. For example, \texttt{UADD8 R0,
R1, R2} will do:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{images/UADD8}
  \caption{A 4 way 8-bit integer add operation.}
  \label{UADD8}
\end{figure}

\section{Multi-Threading}

We have already seen, there are many different ways of increasing CPU
performance:

\begin{itemize}
  \item Increasing the clock frequency
  \item Increasing the \textit{ips} (instructions per clock)
  \item Minimising the impact of accessing memory using a cache
  \item Minimising bubbles with branch prediction
  \item Minimising bubbles using out of order execution
  \item Parallelising instructions (with a superscalar architecture)
  % Weird but it works - look a the compiled version ;) 
  \item[ ] \hspace{-2.3em}And, up to a point
  \item Lengthening the pipeline
\end{itemize}

We have said that increasing the parallelism will speed up our pipeline, but in
order to do this, we need to find enough instructions to safely parallelise at
once. There are multiple ways to do this too:

\begin{itemize}
  \item When we get a branch instruction, do we pause until we evaluate the
  outcome of the branch, or keep issuing instructions? If we're using branch
  prediction, then we can keep issuing instructions.

  \item After a cache miss, we need to wait for a certain amount of time for the
  data we want to come from the memory, over the bus and into the CPU. While
  we're waiting, can we carry on issuing other instructions.

  \item Process instructions in parallel (e.g. with a superscalar architecture).

  \item Write to registers while the previous write is pending using a register
  cache.
\end{itemize}

These ways of increasing parallelism are good, but assume we have only one source
of instructions. However in reality, a CPU will be executing code from multiple
processes at once. What about if we run out of instructions to execute in one
program, we could just context switch to another!

\subsection{Context switches}

In a context switch, the Operating System must load and store a lot of data
about the switching processes:

\begin{mymulticols}
  \begin{itemize}
    \item Process ID
    \item Process state
    \item Program Counter
    \item Stack Pointer
    \item General registers
    \item Memory management information
    \item Open file list (and positions)
    \item Network connections
    \item CPU time used
    \item Parent process ID
  \end{itemize}
\end{mymulticols}

All of this takes time for the processor, and power too, which is a concern on
low power processors.

\subsection{Hardware multithreading}

We could have two PC's, two sets of registers (GPR's on the diagram), two
virtual address mappings etc, and have the CPU support multi threading natively.
This would require the OS to be able to handle multiple processors, since the
easiest way of making it work is to make the one processor core look like two
processor cores (since the inputs and outputs are effectively doubled). See
Figure~\ref{hardware-multithreading-comparison} for examples of the three types
of hardware multithreading.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{images/hyperthreading}
  \caption{The architecture of a multithreaded CPU}
  \label{hyperthreading}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/multithreading}
  \caption{How the three main types of hardware multithreading might execute
  simultaneous threads.}
  \label{hardware-multithreading-comparison}
\end{figure}

\subsubsection{Coarse grain multithreading}

Coarse grain multithreading is when you switch threads whenever the current
thread executes an expensive operation. This could be when there is a cache miss
for example, or whenever the CPU has to wait to execute more instructions.

This type of multithreading has benefits. If the CPU can execute one instruction
per nanosecond ($1GHz$), a cache miss takes $20$ nano seconds, and one cache
miss occurs on average, every hundred instructions, then the number of
instructions per clock cycle is:

\begin{description}
  \item \textbf{Without multithreading}
    \[
      \frac{100ns}{100ns + 20ns \text{(one hundred instructions and one cache miss)}} = 0.8333
    \]
  \item \textbf{With multithreading}
    \[
      \frac{100ns}{100ns + 1ns \text{(one hundred instructions and context switch)}} = 0.99
    \]
\end{description}

Coarse grain multithreading is good, because it requires a minimal change in the
pipeline; only to abort instructions in the `shadow' of a cache miss and the
ability to resume the instruction stream at the correct point afterwards.

Unfortunately, when you do a context switch, a new program runs on the
processor. This is bad, since both the instruction and data caches will suddenly
contain an awful lot of entries that are obsolete, and so most of the cache
requests will be misses. The term `trashing the cache' is used to describe
another thread (or maybe a procedure in the current thread) ruining the cache
hit rate by polluting it with non-relevant data.

\subsubsection{Fine grain multithreading}

The aim of fine grain multithreading is to be able to switch between CPU threads
with as little overhead as possible. This involves interleaving the instructions
of several threads. This results in better overall performance, since the impact
of short stalls such as accessing memory is reduced by executing instructions
from other threads. Each individual thread perceives itself as being executed
slower, but the overall performance is better.

\subsubsection{Simultaneous multithreading (SMT)}

The idea behind SMT is to exploit both instruction level parallelism and thread
level parallelism at the same time. In a superscalar processor (i.e. one that
can execute more than one instruction per clock cycle), we can issue
instructions from different threads in the same cycle.

SMT requires a significant overhead, and is only really feasible to processors
with an out of order execution capability.

\subsubsection{Disadvantages of hardware multithreading}

Threads can `trash' the cache for other threads, since they will probably have
completely different access patterns, so the overall cache performance may
decrease. This will not occur if different threads are accessing the same areas
of memory, but this is a relatively rare case (since processes can only access
disjoint areas of memory) unless a process has multiple threads processing.

Furthermore, there is a significant increase in the complexity of the hardware
required. The thread state, priorities, OS-level information etc must all be
maintained by the processor in hardware to facilitate (fast) context switches.

\subsubsection{Other techniques}

There are more techniques we can apply to thread aware processors in order to
eke out the very best performance from our precious silicon. Here are a few:

\begin{description}
  \item \textbf{Slipstreaming}:\\
    This is when an application is split into two parts; the critical path, and
    all the rest. The critical path will run ahead, and pass the result of
    expensive operations back to the `main thread'
  \item \textbf{Memory prefetching}:\\
    We could compile the application into two parts here too, except one will
    get all the memory accesses, and the other will do everything else. This
    means that the data will always be in the cache when it is needed in the 
    second thread.
  \item \textbf{Speculative execution}:\\
    When we get to a conditional branch in the program, we could spawn two
    threads; one for each path. When we know which is the correct path, we can 
    kill the thread that wasn't executing that path. In this way, we can reduce
    the impact of control hazards.
\end{description}

\section{Multi-Core}

Since Moore's law is slowly coming to an end, chip designers are having to look
to other techniques to increase performance. Cooling is a serious problem now,
since there are so many transistors on chips, that the power density (watts per
unit area) is becoming unsustainable. Smaller transistors have very
unpredictable characteristics, and the architecture of processors is becoming so
complex that it's hard to reason about.

Older problems are becoming worse too; memory is still not getting faster at the
same rate as processors are, and as a result even when the clock speed and the
instructions per second increases on the processor, lots of the time it may be
sat idle.

Lots of solutions have been tried and implemented to increase single core
performance:

\begin{mymulticols}
  \begin{itemize}
    \item Caching
    \item Pipelines
    \item Superscalar processors
    %TODO: Ummm?
    \item Out of order processing
    \item Multithreading
    \item Branch prediction
    %TODO: Ummm?
    \item Forwarding
    %TODO: Ummm?
    \item Register renaming
  \end{itemize}
\end{mymulticols}

However, these all have a limited scalability; the hardware costs increase in a
roughly quadratic manner but the performance increase is sublinear.

Another way to speed up execution is to have multiple CPU cores on one chip.
Multiple simple cores may achieve a higher performance than one complex core
whilst being easier to design.

There are different opinions on how to connect the processors; should they all
access the same memory, or each have their own? From a software point of view,
it is easier to have processors that can access shared memory (since
synchronization is implicit) than having separate memories. Unfortunately,
after a few cores, shared memory becomes harder to attain.

Having more cores does not speed up programs without the programs making use of
the cores explicitly. Threads must be spawned by a program if it wants to
execute on more than one core, however, different programs can run concurrently
on different cores. For example, one core could run an antivirus scan, while
another could run a web browser.

\subsection{The structure of a multi core processor}

In a multicore processor, some components will be shared between processors, and
some won't. Usually, each processor has its own an L1 cache, and sometimes an L2
cache too. You can see a generalisation in Figure~\ref{multicore-arch}.

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{images/multicore-arch}
  \caption{A simplified multicore architecture}
  \label{multicore-arch}
\end{figure}

\subsection{Data coherency and consistency}

Different CPU cores trying to access the same areas of memory can be an issue,
since the caches may not be \textit{coherent}. Coherence is a desired property
of multiple caches all pointing to the same shared resource; each cache should
provide the same view of the resource at all times (at least from the point of
view of the users of the cache).

This shouldn't be a problem, since after all, processes have disjoint memory
spaces, so each process will be running on one core and accessing different
areas of memory. However, threads \textit{do} share memory, and they can run on
multiple cores (and other stuff can update memory, such as DMA etc) so assuming
disjoint memory access across cores is a bad idea.

When we talk about CPU cache coherency, the L1 cache is what we mainly focus on.
Each core will have its own L1 cache, and if a value is updated in one cache
that the others are also holding, then the other caches will be immediately out
of date. Even if we wrote back to memory on every cache write, the other cores'
caches wouldn't be updated. We need to ensure that every core has an up-to-date
cache at all times.

\marginpar{You can think of consistency as representing the model of the
machine as presented to the programmer. If one thread updates a value in memory,
it is expected that all the other threads will also be able to see that update.}

Sometimes, we want to read and update a memory location as a single atomic
operation, and make sure that no other threads can update the same location at
the same time.

\textbf{Sequential consistency} is the idea that memory operations should appear
to execute one at a time, even though they may not do in practice. Leslie
Lamport describes it as:

\begin{aquote}{Leslie Lamport}
  The result of any execution is the same as if the operations of all the
  processors were executed in some sequential order, and the operations of each
  individual processor appear in this sequence in the order specified by its
  program.
\end{aquote}

Since lower down the technology stack of a running program, instructions may be
executed out of order (for example in Java), the compiler must insert special
instructions such as the following to ensure that consistency is maintained:

\begin{description}
  \item \textbf{Fence}:\\
    A fence will make sure each memory access before the fence completes before
    new ones are started.
  \item \textbf{Barrier}:\\
    All threads in the program must reach the barrier before any of them can
    continue executing.
  \item \textbf{Lock}:\\
    Only one thread can enter a section of the program that is protected by a 
    lock at any one time. That section may be referred to as atomic.

    Hardware support is usually required to implement locks, since normal memory
    reads and writes cannot be guaranteed to be atomic without, guess what\dots
    locks!
\end{description}

There is some Instruction Set Architecture support for synchronization, such as
atomic `compare and swap' instructions conditional load and stores (based on if
the memory location has mutated since it was last accessed), and transactional
memory.

\textbf{Transactional memory} allows you to read and write to memory with no
restrictions, however once the operation is completed, the transaction is
checked to see if it conflicted with any other transaction, and it is rolled
back if it did (and started again).

\subsubsection{Coherence protocols}
\label{snooping}

A scheme where each core would know exactly when other cores are caching the
same data is infeasible as it is would be far too complex (and probably slow).
As an alternative to this, each cache \textit{snoops} on the other caches for
activity related to its own cache. This is most simply implemented by routing
all data through a bus so every cache can see the activity of other caches.

There are two very simple snooping protocols in use:

\begin{description}
  \item \textbf{Write Update}:\\
    When a core writes a value to memory, it is updated in its local cache. The
    cache then broadcasts the address and new data to the bus and all the
    snooping caches update their own copy.
  \item \textbf{Write Invalidate}:\\
    A core that wants to write to an address writes to its own cached copy, but
    also sends a `write invalidate' message that will tell the other cores
    to invalidate the cache line that they have stored. Any read to that address
    will now miss.
\end{description}

In both schemes, the bus makes sure that only one core can use the bus at one
time, so that simultaneous writes do not occur. Though the first option (write
update) might look fastest, it isn't always, since it will update the cache when
it might not be needed (e.g. two writes to the same cache in a row would require
two updates to the other caches, but only one invalidate using write
invalidate). This can also happen when you're writing to different words in the
same block of a multi word cache (most caches are multi word remember!).

Unfortunately, this can happen often due to spatial and temporal locality (the
principles that make CPU caches work), and bus bandwidth is a precious
commodity, especially in shared memory multi-core chips. Invalidate protocols
have been modelled to use less bandwidth, and so these are more commonly used.

In both of the above schemes, merely knowing if other CPU caches hold the value
is enough to know whether to send a message on the bus in the first place.

If we use the invalidation scheme, then if the writes don't also get written
through to memory, then when other caches read from the memory, they will read
old values, so we need a protocol to handle this\dots

\textbf{MESI Protocol}

This is a practical multi-core invalidation protocol which tries to minimise the
bus usage. It also implements a `copy back' scheme where the main memory or L2
cache is not updated until a `dirty' cache line is displaced. 

As a result of this, cache lines have two state bits, meaning they can be in the
following states:

\begin{description}
  \item \textbf{Modified}:\\
    The cache line has been modified and is different from the main memory (i.e.
    it's a dirty copy).
  \item \textbf{Exclusive}:\\
    The cache line is exactly the same as main memory and it's the only copy.
  \item \textbf{Shared}:\\
    It's the same value as the main memory, but other copies may be in other
    caches which may differ.
  \item \textbf{Invalid}:\\
    The cache line data is not valid.
\end{description} 

Another protocol called \texttt{MOESI} has another state:

\begin{description}
  \item \textbf{Owned}:\\
    The cache line has been modified and is different from memory. There are
    other copies in other caches.
\end{description}

\marginpar{Owned means that we don't have to copy back to main memory on writes.
}

The changes of state of the cache line are dependent on the memory access
events, which can be either from local core activity or snooping from other
cores. The cache line state will only be effective if its address matches the
address of the memory event.

The state changes are as shown in Figure~\ref{MESI}.

\marginpar{`RWITM' means Read With Intent To Modify.}

\begin{figure}[H]
  \includegraphics[width=0.45\textwidth]{images/MESI1}
  \includegraphics[width=0.45\textwidth]{images/MESI2}
  \caption{MESI - local cache view (left) and snooping cache view (right)}
  \label{MESI}
\end{figure}

The consensus is that snooping protocols don't scale well beyond around 16
cores, since the bus connecting the cores becomes saturated. We could have a
hierarchy of buses, but this is complicated.

Instead, we can create directory based protocols. If we use distributed
hardware, we can have a directory listing what data other caches have, and
caches can talk to each other in a point-to-point manner. Since there is more
traffic required for this between the cores, the bus is often replaced with a
network on a chip.

The cache lines can be in three states that are like those of MESI:

\begin{itemize}
  \item \textit{Invalid}
  \item \textit{Shared} (coherent with the main memory)
  \item \textit{Modified} (value is changed and no other copies are anywhere)
\end{itemize}

The directory can have three states too:

\begin{itemize}
  \item \textit{NC} (Not Cached; the cache line isn't present anywhere)
  \item \textit{Shared} (the cache line is present in at least one core)
  \item \textit{Modified} (the cache line is modified and is in one other core)
\end{itemize}

The directory also stores a \textit{sharing vector} describing what core holds
the same cache line, which can be seen in Figure~\ref{directory-protocol}.

\begin{wrapfigure}{o}{0.4\textwidth}
  \centering
  \vspace{-2em}
  \includegraphics[width=0.35\textwidth]{images/directory-protocol}
  \caption{How data might be organised in a directory protocol}
  \label{directory-protocol}
\end{wrapfigure}

If we have a central directory, then it might become the bottleneck we were
trying to avoid with a bus. To stop this, we could use a distributed directory,
where each core has its own directory, this requires an extra flag `home' which
indicates the data is stored locally.

To sum it up, directory protocols are:

\begin{itemize}
  \item More scalable (better for CPU's with more cores).
  \item Use p2p messaging to simplify interconnection and allow parallel
    communication.
  \item Each core doesn't have a global view of the caches, we need to store the
    state separately in the directory.
  \item Control is more complex, but performance is better overall.
\end{itemize} 

\subsection{On-chip interconnects}

So far, we've only really considered buses as a way of communicating between
cores, however we could use a network on a chip (NoC) to communicate. Things we
need to consider about networks on a chip are:

\begin{mymulticols}
  \begin{itemize}
    \item Bandwidth
    \item Latency
    \item Congestion
    \item Fault tolerance
    \item Area ($mm^2$)
    \item Power dissipation
  \end{itemize}
\end{mymulticols}

There are three important features of a NoC; topology (how the cores and network
infrastructure is organised), routing (how the traffic moves around) and
switching (how the traffic moves from once component to another).

The medium through which data is transmitted is a bus. Buses are single usage at
any one time and are controlled by a clock that divides its use into time slots.
Transactions are often split, since a message will be sent in one slot, and th
reply received in another later slot (slots can be used in between these two
events).

Networks on a chip can take many different forms though:

\begin{center}
  \begin{tabular}{| m{2cm} | m{5cm} | c |}
    \hline
    Crossbar & You can connect $n$ inputs to $n$ outputs & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/crossbar}
      \end{center}
    \end{minipage} \\ \hline
    Ring & Simple, but low bandwidth and variable latency & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/ring}
      \end{center}
    \end{minipage} \\ \hline
    Tree & Variable bandwidth and latency (depth etc), may be unreliable & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/tree}
      \end{center}
    \end{minipage} \\ \hline
    Fat tree & Faster, more reliable, but uses more resources than a tree & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/fat-tree}
      \end{center}
    \end{minipage} \\ \hline
    Mesh & Okay bandwidth, variable latency, but good for large systems because
    of the layout & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/mesh}
      \end{center}
    \end{minipage} \\ \hline
  \end{tabular}
\end{center}

\subsubsection{Routing}

There are three types of routing we're going to look at:

\begin{center}
  \begin{tabular}{| m{2cm} | m{5cm} | c |}
    \hline
    Minimal &
    \begin{itemize}
      \item Always selects the shortest path
      \item Packets move closer at every step
      \item But are more likely to be blocked
    \end{itemize} & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/minimal}
      \end{center}
    \end{minipage} \\ \hline
    Oblivious &
    \begin{itemize}
      \item Unaware of network state
      \item Packets take a fixed path
      \item Very simple and deadlock free
      \item Prone to contention
    \end{itemize} & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/oblivious}
      \end{center}
    \end{minipage} \\ \hline
    Adaptive & 
    \begin{itemize}
      \item Aware of network state (moves packets to avoid contention)
      \item Higher performance
      \item More area and power required
      \item Deadlock prone (more hardware needed)
      \item Rarely used in NoC's
    \end{itemize} & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/adaptive}
      \end{center}
    \end{minipage} \\ \hline
  \end{tabular}
\end{center}

\subsubsection{Switching}

There are two types of packet switching we'll look at, store and forward and
wormhole switching. The premise is that data is split into small packets, some
extra information is added to enable them to get to their destination and they
are sent into the network. This allows time-multiplexing of network resources
and is good for performance (especially in short messages). Packets are also
split into flits, which are teeny packets (see Figure~\ref{flits}).

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\linewidth]{images/flits}
  \caption{How a packet is split up into flits.}
  \label{flits}
\end{figure}

\begin{center}
  \begin{tabular}{| m{2cm} | m{5cm} | c |}
    \hline
    Store and forward &
    \begin{itemize}
      \item A packet is not forwarded until all its phits arrive to each node
      \item Failure detection happens on the fly
      \item But performance is low
      \item And large buffers are required
    \end{itemize} & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/store-and-forward}
      \end{center}
    \end{minipage} \\ \hline
    Wormhole &
    \begin{itemize}
      \item A packet is forward as soon as its head arrives
      \item Performance is better than store and forward
      \item But fault detection is only possible at the destination
      \item Less hardware
    \end{itemize} & 
    \begin{minipage}{.3\textwidth}
      \begin{center}
        \includegraphics[width=0.5\linewidth]{images/wormhole}
      \end{center}
    \end{minipage} \\ \hline
  \end{tabular}
\end{center}

\section{Virtualization}

\begin{wrapfigure}{R}{0.3\textwidth}
  \centering
  \includegraphics[width=0.25\textwidth]{images/types-of-virtualization}
  \caption{The different types of virtualization. PIN is a tool to annotate
  binary executables if you hadn't heard of it.}
  \label{types-of-virtualization}
\end{wrapfigure}

Virtualization isolates the details of the hardware from the software that uses
it. You can break virtualization down into two broad categories:

\begin{description}
  \item \textbf{Process Virtualization}:\\
    Run a process under the control layer of software, e.g. the JVM running Java
    bytecode.
    \marginpar{Here, a ``layer of software'' can mean a lot of things, but
    generally one or more processes.}
  \item \textbf{System Virtualization}:\\
    Run a whole OS under the control of a layer of software, e.g. VMware.
\end{description}


This is useful, since virtualization can translate between technologies
(different instruction sets, system calls etc), change the level of abstraction
(providing garbage collection, debugging etc) and even make the system resources
look different (emulate CD drives, reduce the amount of RAM for a virtual
machine).

\marginpar{There's a really cool debugging technique called Reverse Debugging
that is made possible my virtualization. When you hit a breakpoint, the debugger
lets you step \textit{back} through the code. This is often implemented by
having the VM keep track of what each instruction did and reversing the
operation each time you step back. Different granularities can be implemented;
the debugger could save resources by just implementing a function reset feature
that discards the current stack frame and lets you run the function again while
looking at exactly what's happening.}

For example, the JVM interprets byte code, and maps it onto the host OS's API.
Rosetta translates PowerPC binaries to x86 ones on the fly.

\subsection{The details of virtualization}

A virtualised OS runs on top of a Virtual Machine Monitor (VMM) or Hypervisor.
This often (but not always, since it is sometimes one itself) sits on top of the
\textit{actual} operating system running on the machine. The VMM will handle
physical resource access for the guest OS since it runs in a privileged mode,
and it makes sure the guest OS is isolated from other resources. These resources
include:

\begin{mymulticols}
  \begin{itemize}
    \item Timers
    \item CPU registers
    \item CPU flags (e.g. interrupt enable)
    \item Device control registers (DMA, interrupt IO etc)
    \item Memory mapping (page table etc)
  \end{itemize}
\end{mymulticols}

Often, when a guest OS tries to access resources its not allowed to, it will
trigger a trap instruction on the VMM, which allows the VMM to check the bounds
of access for that instruction/OS and proceed accordingly. Since the VMM does
the privileged operations, the guest OS can be unprivileged.

This can be tricky sometimes though; some instructions behave differently
according to what mode they are in, so the VMM must be able to handle that.

If the guest OS is virtualization aware, then it is much easier to build a VMM,
since the guest OS can call the VMM specifically for privileged operations,
cooperate with the VMM over shared page tables and call the VMM for IO.

\subsection{Operations on VM's}

When you start a VM, the hypervisor will save the current registers, load the
VM's initial registers and jump to the new VM's PC address. Likewise, when a VM
is stopped, the hypervisor will save its registers into its own memory space. It
is important to note that VM's are stopped and started all the time so that the
CPU (and other resources) can be shared.

When the VM is stopped, its memory and IO state is also retained as well as its
CPU registers. It is best to stop a VM when its IO is \marginpar{Quiescent means
to be `in a state or period of inactivity or dormancy'}\textit{quiescent}, so
that we don't have to save loads of IO buffers etc.

Once we've stopped a VM, we can freeze it by saving all its state into a file.
This file tends to be large, since it contains the whole operating system, its
installed applications and the state such as the registers etc.

Because virtual machines can be stored as files, there are a number of things we
could do with them:

\begin{mymulticols}
  \begin{itemize}
    \item Move a VM onto a different machine
    \item Snapshot the state of a VM and roll it back later
    \item Archive a VM into a database or secondary storage
    \item Quickly start an archived VM (rapid provisioning)
    \item Live migration (we'll get on to this)
    \item Load balancing (and this)
  \end{itemize}
\end{mymulticols}

\subsubsection{Live migration}

Wouldn't it be good to be able to move a VM from one machine to another without
pausing execution of the VM? That means we could change the physical machine a
VM is running on (maybe to repair or upgrade the old one) with minimal
interruption for the end user.

I'm going to use Wikipedia's explanation for this, since it's really good:

\begin{aquote}{\url{http://en.wikipedia.org/wiki/Live_migration}}
  \textbf{Warm-up phase}
  
  In pre-copy memory migration, the Hypervisor typically copies all the memory
  pages from source to destination while the VM is still running on the source.
  If some memory pages change (become `dirty') during this process, they will be
  re-copied until the rate of re-copied pages is not less than page dirtying
  rate.

  \textbf{Stop-and-copy phase}

  After the warm-up phase, the VM will be stopped on the original host, the
  remaining dirty pages will be copied to the destination, and the VM will be
  resumed on the destination host. The time between stopping the VM on the
  original host and resuming it on destination is called `down-time', and ranges
  from a few milliseconds to seconds according to the size of memory and
  applications running on the VM. There are some techniques to reduce live
  migration down-time, such as using probability density function of memory
  change.
\end{aquote}

\subsubsection{Load balancing}

Say we had two physical machines and six VM's. The six VM's were distributed
equally between two people, one of which did parallel text mining, and the other
ran a twitter bot. Since we wanted to be fair, we assigned them one physical box
each, with their three VM's on it, however, the text mining guy was always
running at $100\%$ CPU utilisation, while the other guy barely got over $2\%$.

This is where load balancing can be helpful; we could have the Hypervisor move
VM's between physical machines based on the relative load on the physical
machines. This would work independently of the application of the running VMs.
This way, our text mining guy can (in theory) have $98\%$ of the CPU power of
the second physical machine as well as near $100\%$ of the first one too!

We can also use a similar technique to keep services highly available, if we
regularly copy the state of one VM onto another physical machine, then the
secondary one is effectively a standby backup in case the first one crashes.

\section{Permanent Storage}

\subsection{Disks and filesystems}

Broadly, there are three main categories of permanent storage media:

\begin{description}
  \item \textbf{Write Once, Read Many} (WORM):\\
    This includes CD-ROM, DVD-ROM, and some Blu-ray Disks. Once you've written,
    you can't write over it.
  \item \textbf{Write Many, Read Many}:\\
    This includes hard drives, tape drives etc. The writes are fully reversible
    for the purposes of the computer. \marginpar{Of course, the writes aren't
    literally fully reversible because of wear etc, but they are near enough.}
  \item \textbf{Write (not too) Many, Read Many}:\\
    Rewritable CD/DVD's have hundreds to thousands of write cycles, and flash
    memory has more, from the thousands to the low millions. These devices will
    wear slightly on each write, making them less effective.
\end{description}

\subsubsection{Hard drives}

\begin{wrapfigure}{o}{0.2\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{images/hard-drive}
  \caption{The structure of a `traditional' spinning disk Hard Drive}
  \label{hard-drive}
\end{wrapfigure}

Hard drives consist of multiple magnetic disks (platters) laid on top of each
other that spin around and can be written to or read from by a `head'. Each
platter can store around $2\si{\tera\byte}$ at the moment, and hard drives often
have around four of them, meaning that the largest hard drives can store about
$8\si{\tera\byte}$ of data. The platters rotate at one of four speeds, $5400RPM,
7200RPM, 10000RPM$ or $15000RPM$.

Hard drives are power hungry devices compared to lots of other components in the
computer since they need power for spinning the platters, moving the heads,
reading and writing, the chips on the hard drive that control the IO etc.

Hard drives are very slow in terms of data bandwidth and latency compared to
other forms of memory storage such as RAM. The following terms are used to
quantify hard drive performance characteristics (refer to Figure~\ref{hd-config}
if you've not heard of some of the terminology before):

\begin{description}
  \item \textbf{Seek Time}:\\
    This is the time it takes for the head to reach the target track on the
    platter.
  \item \textbf{Search Time}:\\
    This is the time for the target sector to arrive under the head.
  \item \textbf{Transfer Rate}:\\
    This is the amount of data that can be read per unit time. It depends on
    a lot of factors such as where data is on the disk and access patterns of
    the data, though the transfer rate is usually given as a `sustained transfer
    rate' in $\si{\mega\byte\per\second}$.
  \item \textbf{Disk Access Time}:\\
    The total time it takes to access data on the disk:
    \[
     \text{Disk access time} = \text{Seek time} + \text{Search time}
                                + \text{Transfer time}
    \]
\end{description}

It is sometimes important to compute the average access time of a hard drive
given the seek time, rotation speed, transfer speed and sector size. For
example, if a sector is $512\si{\byte}$, the seek time is
$8.5\si{\milli\second}$, the disk rotates at $7200RPM$ and the transfer speed is
$177\si{\mega\byte\per\second}$, then we can compute the access time to be:

\marginpar{This assumes that the hard drive will have to do $0.5$ rotations to
reach the correct sector (this is the average number of rotations when you think
about it).}

\[
  \begin{split}
    \text{Search Time (\si{\milli\second})} &= \frac{0.5\text{ rotations} \times 60}{7200RPM}\\
                                            &= 4.16\si{\milli\second}\\
    \text{Transfer Time (\si{\milli\second})} &= \frac{512\si{\byte}}{177\times10^6\si{\byte\per\second}}\\
                                              &= 2.89\si{\micro\second}\\
    \text{Disk access time} &= \text{Seek time} + \text{Search time}
                                + \text{Transfer time}\\
                            &= 8.5\si{\milli\second} + 4.16\si{\milli\second} + 2.89\si{\micro\second}\\
                            &= 12.66\si{\milli\second}
  \end{split}
\]

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{images/hd-schematic}
  \caption{The internal structure of a hard drive and the layout of its disk.}
  \label{hd-config}
\end{figure}

\marginpar{See slides 9-14 on the first of the storage lecture notes for a
pictorial example.}

Sometimes, when the Operating Systems wants to read a file off the hard disk,
the file will be split over multiple sectors. In order to read the file, the
hard drive will need to move to the head to each sector to read the whole file.
If the sectors are physically distant from each other (say one was on the very
inner track of the disk, and one was on the outermost track), then the hard
drive may have to `waste' rotations in order to move the head to the correct
track. An internal processor in the hard drive will re-order the operating
system's sector requests so that they are in the most efficient order for
retrieval.

\subsubsection{RAID}

RAID stands for a Redundant Array of Independent Disks, which is a type of
storage virtualization.

Hard disks are often too slow because of a high seek time, a high search time
and a low sustained transfer rate. We can combat the first two problems by
having multiple platters on the disk so there are more sectors per cylinder (so
the head has to move less), and increasing the speed of rotation will decrease
the search time.

A low sustained transfer rate cannot be easily increased though. One solution is
to \textit{stripe} a file over multiple disks. This is called \texttt{RAID 0}.
If a file can be split up into nine sections, we could put it on four disks like
so, and speed up our sustained transfer rate by around four times:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{images/raid0}
  \caption{RAID 0 gives higher speed, but lower reliability.}
  \label{raid0}
\end{figure}  

If a disk did break in this situation, then it'd be bad since whole swathes of
the file would disappear.

To avoid this, we could use \texttt{RAID 1}, which mirrors the data onto
another disk. This means we would have four copies of the same data, but
\textit{all} of our hard drives would have to fail before we lost any data:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{images/raid1}
  \caption{RAID 1 doesn't increase speed, but it does increase reliability}
  \label{raid1}
\end{figure}

A half-way-house solution between these two, would be \texttt{RAID 10}. This is
a nested configuration, which still stripes the data, but keeps copies of the
stripes. It can tolerate disk failure as long as no \texttt{RAID 1} mirror loses
all of its drives:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.6\textwidth]{images/raid10}
  \caption{RAID 10 gives higher speed, and higher reliability.}
  \label{raid10}
\end{figure}  

\marginpar{See my \texttt{COMP28512} notes for more information on hamming codes
and parity bits.}

We can use error correction (e.g. hamming codes and parity bits) to provide
redundancy \textit{and} striping as done using \texttt{RAID 2,3} and \texttt{4}.
\texttt{RAID 5} and \texttt{6} both distribute their parity bits over other
drives.

\begin{figure}[H]
  \centering
  \begin{tabular}{cc}
    \includegraphics[width=0.5\textwidth]{images/raid2} &
    \includegraphics[width=0.5\textwidth]{images/raid3}\\
    Bit Striping and Hamming Codes & Byte Striping and Parity Bits\\
    \includegraphics[width=0.5\textwidth]{images/raid4} &
    \includegraphics[width=0.5\textwidth]{images/raid5}\\
    Block Striping and Parity & Block Striping and Distributed Parity\\
    \multicolumn{2}{c}{\includegraphics[width=0.6\textwidth]{images/raid6}}\\
    \multicolumn{2}{c}{Double Distributed Parity}
  \end{tabular}
  \caption{RAID 2-6}
  \label{raid2-6}
\end{figure}

Popular other combinations of \texttt{RAID} include \texttt{RAID 50} and
\texttt{RAID 160}. Remember that \texttt{RAID 160} is \texttt{RAID 1} inside
\texttt{RAID 6} inside \texttt{RAID 0}.

What happens if a disk fails in the following situations:

\begin{description}
  \item \textbf{RAID 0} You lose all your data (and hope there's another RAID
    layer).
  \item \textbf{RAID 1} You just hot-swap the failed disk.
  \item \textbf{RAID 2-6} Operate in a degraded mode:
    \begin{itemize}
      \item If a data drive failed, then every read must be reconstructed.
      \item If a parity drive failed, then there is a low performance impact
        (while the system recomputes the parity bits with a new drive).
    \end{itemize}
\end{description}

Most operating systems support some kind of \texttt{RAID}, or you can buy a
dedicated hardware controller.

\subsubsection{Solid State Drives}

Solid State Drives (SSD) are made of flash memory. They have a Floating Gate
Field Effect Transistor that can store 0's and 1's. It is possible that they
will become multi level (i.e. store two bits) in the future.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\textwidth]{images/fgfet}
  \caption{A Floating Gate Field Effect Transistor}
  \label{fgfet}
\end{figure}

Solid state storage has a data retention time of around ten years, but will also
wear out with write cycles (and so the performance decreases too). X-rays can
affect the data on the drive (so don't take it through airport security).

Error correcting codes, block remapping and wear-levelling are all used to
mitigate these flaws though. Wear levelling is when logical block addresses are
mapped to physical addresses differently over time so that specific blocks
aren't worn out.

SSD's are much faster than hard drives though, since they don't have moving
parts, so the data access is much faster (the latency is around
$0.1\si{\milli\second}$ or less. Their sustained read and write rates are much
faster too. You pay more per gigabyte though, around 14 times more at current
rates.

\subsubsection{Storage Virtualization}

Classically, we have a file system that maps from our logical thinking of how
files are ordered onto the disk. File systems do not naturally span onto
multiple drives though. RAID enables us to break this mapping by striping and
mirroring. It's kind of similar to virtual memory in the sense that the file
path doesn't have much of a relation to where the data is actually stored.

A \textit{volume group} is a set of drives in a pool, and storage space in such
a group is divided into \textit{physical extents}, which are usually of the same
size. A \textit{logical volume} will be made of some of the physical extents.

These abstractions let us add more drives, extend partitions within drives, take
a snapshot of a file system etc. This greater control can be used to great
advantage. For example, with Linux:

\begin{description}
  \item \texttt{/} Mostly read, we want fast read seeks and transfer rates.
  \item \texttt{swap} Reads and writes, but we want a high bandwidth. This can
  tolerate data loss better than any other part of the hard drive (since its
  only RAM).
  \item \texttt{/opt} Infrequent access
  \item \texttt{/var} Infrequent access
\end{description}

This means we could mirror \texttt{/}, stripe the \texttt{swap} and allocate the
spare space to \texttt{/opt} and \texttt{/var}.

\subsubsection{Storage Area Networks (SAN)}

Storage Area Networks are used to decouple compute servers from storage. They
allow for storage specific functions to be localised to and optimised for them.
This means we can share disk resources across servers, rapidly migrate disk
images, share common subsets of file systems (e.g. virtual machine images) and
more. One downside is the decreased bandwidth and increased latency, but with
modern network protocols, this isn't as much of an issue.

ZFS is a volume aware file system. It protects against losing files, running out
of space, corruption of data etc by being very flexible and having lots of ECC.
These are implemented by copy-on-write, simple rollback and recovery, wear
leveling, self checking and healing, sumchecking and more.

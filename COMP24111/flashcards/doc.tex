\documentclass[frontgrid]{flacards}
\usepackage{color}
\usepackage{tabularx}

\definecolor{light-gray}{gray}{0.75}

\newcommand{\frontcard}[1]{\textcolor{light-gray}{\colorbox{light-gray}{$#1$}}}
\newcommand{\backcard}[1]{#1} 

\newcommand{\flashcard}[1]{% create new command for cards with blanks
    \card{% call the original \card command with twice the same argument (#1)
        \let\blank\frontcard% but let \blank behave like \frontcard the first time
        #1
    }{%
        \let\blank\backcard% and like \backcard the second time
        #1
    }%
}

\begin{document}

\pagesetup{2}{4} 

\card{
	What are the \textbf{advantages} of a nearest neighbour classifier?
}{
	- Very accurate\\
	- No learning process
}

\card{
	What are the \textbf{disadvantages} of a nearest neighbour classifier?	
}{
	- Very computationally expensive for every classification\\
	- Complexity depends on the number of dimensions
}

\card{
    What is the most important concept in machine learning?
}{
    Never assume that you have all the data.
}

\card{
    What are the three `ingredients' of a machine learning algorithm?
}{
    The model, the error function and the learning algorithm.
}

\card{
    What does this equation calculate?
    \[a = \sum\limits_{i=1}^{F}x_iw_i\]
}{
    The activation of the perceptron
}

\card{
    What is the perceptron learning rule?
}{
    $newWeight = oldWeight + 0.1 \times (trueLabel - output) \times input$
}

\card{
    What does this equation calculate?
    \[a = \frac{1}{1 + exp(-\sum\limits_{i=1}^d w_i x_i)}\]
}{
    The activation of the perceptron for non-linearly separable data
}

\flashcard{
    Decision trees are good at handling \blank{categorical} data but worse at handling \blank{continuous} data.
}

\card{
    What does this equation calculate?
    \[H(X) = -\sum\limits_i p(x_i)\log_2 p(x_i)\]
}{
    The entropy of a variable $X$
}

\flashcard{
    The `information' contained in a variable is called the \blank{entropy}.
}

\card{
    Explain the process of cross validation.
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        1. & Break the data evenly into $N$ chunks\\
        2. & Leave one chunk out\\
        3. & Train on the remaining $N-1$ chunks\\
        4. & Test on the chunk you leave out\\
        5. & Repeat until all chunks have been used to test\\
        6. & Plot the average and error bars for the $N$ chunks\\
    \end{tabularx}
}

\card{
    What factors should affect our decision on the best learning algorithm to use?
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        1. & Accuracy\\
        2. & Training time and space complexity\\
        3. & Testing time and space complexity\\
        4. & Interpretability\\
    \end{tabularx}
}

\card{
    What is the ensemble approach to machine learning?
}{
    Select a class of models, fit multiple models to training data (called base learners), use the models as a committee to vote on testing data.
}

\card{
    Briefly describe bootstrapping
}{
    Bootstrapping is the process of generating multiple data sets from an original.
}

\card{
    On average, what is the percentage of data points that are left unselected?
}{
    36.8\%
}

\card{
    Explain bagging
}{
    Generate $m$ bootstraps and train a model on each one. When the testing data arrives a simple majority vote takes place.
}

\card{
    Explain boosting
}{
    Get a data set, take a bootstrap and train a model on it. See which examples the model got wrong then upweight those `hard' examples and downweight the `easy' ones. Now go back to training a model, but now you have a weighted bootstrap.
}

\card{
    What type of classifier models a classification rule directly and models the probability of class memberships based on input data?
}{
    A discriminative classifier
}

\card{
    What type of classifier makes a probabilistic model of data within each class?
}{
    A generative classifier
}

\card{
    What type of classifier uses probabilities to classify data?
}{
    A probabilistic classifier
}

\card{
    What is the formula to work out $P(c|X')$ Where $c$ is a class and $X'$ is an example?
}{
    \[P(c|X') = [P(x_1|c)P(x_2|c)....P(x_n|c)]P(c)\]
    Where $x$ is a feature in the example.
}

\card{
    What is the formula to work out a Gaussian model?
}{
    \[\frac{1}{\sigma \sqrt{2\pi}}exp(-\frac{(x - \mu)^2}{2 \times \sigma^2})\]
}

\card{
    What are the two data representation methods that we talk about in clustering analysis?
}{
    Data matrices and distance matrices.
}

\card{
    What is the formula to work out Minkowski distance.
}{
    \[d(x,y) = \sqrt[p]{(x_1 - y_1)^p + (x_2 - y_2)^p...+ (x_n - y_n)^p}\]
}

\card{
    What is the formula for Manhattan distance?
}{
    \[d(x,y) = |x_1 - y_1| + |x_2 - y_2| + |x_n - y_n|\]
}

\card{
    What is the formula for Euclidean distance?
}{
    \[d(x,y) = \sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2...+ (x_n - y_n)^2}\]  
}

\card{
    What is the cosine measure equation
}{
    \[\frac{x_1y_1+...+x_ny_n}{\sqrt{x_1^2+...+x_n^2}\sqrt{y_1^2+...+y_n^2}}\]
}

\card{
    What is the formula for the distance between symmetric binary attributes?
}{
    \[d(x, y) = \frac{b + c}{a + b + c + d}\]
}

\card{
    What is the formula for the distance between asymmetric binary attributes?
}{
    \[d(x, y) = \frac{b + c}{a + b + c}\]
}

\card{
    Briefly explain the partitioning clustering approach
}{
    Various partitions are constructed and then evaluated by some criterion, r.g. minimizing the sum of square distance cost. Typical methods are k-means, k-medoids, CLARANS.
}

\card{
    Briefly explain the hierarchical clustering approach
}{
    Create a hierarchical decomposition of the set of data (or objects) using some criterion. Typical methods are Agglomerative, Diana, Agnes, BIRCH, ROCK.
}

\card{
    How does the K-means clustering algorithm work?
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        1. & Pick $K$ random seed points (from the data)\\
        2. & Assign each data object to the cluster with the nearest seed point\\
        3. & Compute the mean points of the clusters (aka centroids)\\
        4. & Go back to step 1, but the seed points are now the means calculated in step 3. Do this until the assigned sets don't change between iterations.\\
    \end{tabularx}
}

\card{
    What is the runtime of the K-means algorithm?
}{
    \[O(tKn)\]\\
    Where $t$ is the number of iterations\\
    $K$ is the number of clusters\\
    $n$ is the number of objects
}

\card{
    What are some problems with the K-means algorithm?
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        - & The initial seed points can cause `local optimum' clusters, which may not be representative of the whole data.\\
        - & If a cluster has a non-convex (i.e. concave) shape, then it won't be detected.\\
        - & Unable to classify categorical data (unless you modify the algorithm).\\
        - & It's hard to evaluate the performance of the algorithm,\\
    \end{tabularx}
}

\card{
    What are the two sequential strategies used to construct a tree of clusters?
}{
    Agglomerative and divisive
}

\card{
    Explain the single link method of measuring the distance between clusters
}{
    Use the smallest distance between an element in one cluster and an element in another.
}

\card{
    Explain the complete link method of measuring the distance between clusters
}{
    Use the largest distance between an element in one cluster and an element in another.
}

\card{
    Explain the average method of measuring the distance between clusters
}{
    Find the average distance between the points in the two clusters and use that.
}

\card{
    How does the agglomerative hierarchical algorithm work?
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        1. & Convert the object attributes into a distance matrix, and make each object a cluster (of size 1).\\
        2. & Merge the two closest clusters together.\\
        3. & Update the distance matrix to take into account the new cluster.\\
        4. & Repeat from step two until we've got the desired number of clusters.\\
    \end{tabularx}
}

\card{
    What are some weaknesses of the agglomerative approach?
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        - & It is very sensitive to noise.\\
        - & It is less efficient than k-means clustering, with a runtime of $O(n^2)$\\
    \end{tabularx}
}

\flashcard{
    In order to calculate the F-ratio index we divide the \blank{intra-cluster}
    variance by the \blank{inter-cluster} variance.
}

\card{
    What is the formula to calculate the F-ratio index?
}{
    \[F(m) = \frac{mSSW(m)}{SSB(m)} = \frac{m\sum\limits_{i=1}^m\sum\limits_{j=1}^{n_i}d^2(x_{ij},c_i)}{\sum\limits_{i=1}^mn_id^2(c_i,c)}\]
}

\card{
    \[F(m) = \frac{mSSW(m)}{SSB(m)} = \frac{m\sum\limits_{i=1}^m\sum\limits_{j=1}^{n_i}d^2(x_{ij},c_i)}{\sum\limits_{i=1}^mn_id^2(c_i,c)}\]
}{
    \begin{tabularx}{0.32\textwidth}{l X}
        $m$ & The number of clusters generated by the algorithm.\\
        $n_i$ & The number of data points in the $i$th cluster.\\
        $c_i$ & The centroid for the $i$th cluster.\\
        $x_{ij}$ & The $j$th datapoint in cluster $c_i$.\\
        $c$ & The mean centroid for the whole data set.\\
        $d(x,y)$ & The distance function we're using.\\
    \end{tabularx}
}

\card{
    What is additive smoothing used for? Give an equation for it.
}{
    When a $P(x|\omega_i)$ has a probability of $0$ (it's $0$ for just class $i$
    , additive smoothing can ensure the probability of the data object isn't
    $0$.
    \vspace{1em}
    \[
        P(x|\omega_i) = \frac{N_{i,\omega}+\alpha}{N_i + \alpha d}
    \]
    Where $N_i$ is the count of that feature, $N_{i,\omega}$ is the count for
    that class, $\alpha$ is the smoothing parameter (1 for laplace smoothing)
    and $d$ is the number of dimensions.
}

\card{
    How do you find the number of clusters with the smallest F-ratio?
}{
    Run the clustering algorithm ranging from $K=2$ up to $n$. Then find the
    F-ratio index for each $K$ value, and use  the value with the lowest F-ratio
    index.
}
\end{document}

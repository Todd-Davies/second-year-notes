\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Machine Learning}{}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Nearest Neighbour Classifier}{}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Finding the distance between two $n$-dimensional points}{}{subsection.2.1}}
\newlabel{scala_nn}{{1}{}{Scala Euclidean Distance}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}Scala Euclidean Distance}{}{lstlisting.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Computing the nearest neighbour}{}{subsection.2.2}}
\newlabel{nearestNeighbour}{{2}{}{Scala Nearest Neighbour}{lstlisting.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}Scala Nearest Neighbour}{}{lstlisting.2}}
\gincltex@bb{diagrams/knn-large.tex}{0}{0}{172.46927}{63.63278}
\gincltex@bb{diagrams/knn-boundary.tex}{0}{0}{44.2132}{58.38638}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multiple nearest neighbours (K-NN)}{}{subsection.2.3}}
\newlabel{nearestNeighbourk}{{3}{}{Scala Nearest Neighbour}{lstlisting.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}Scala Nearest Neighbour}{}{lstlisting.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces How K affects classification accuracy. The item being classified is the filled square and nearest neighbours are filled, and other elements are left unfilled. When K=3, it would be classified as square, when K=5, it'd be classified as a circle.\relax }}{}{figure.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:knn-large}{{1}{}{How K affects classification accuracy. The item being classified is the filled square and nearest neighbours are filled, and other elements are left unfilled. When K=3, it would be classified as square, when K=5, it'd be classified as a circle.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The dotted line represents the decision boundary for a KNN classifier\relax }}{}{figure.caption.6}}
\newlabel{fig:knn-boundary}{{2}{}{The dotted line represents the decision boundary for a KNN classifier\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Overfitting}{}{subsection.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Linear Classifier}{}{section.3}}
\newlabel{linearClassifier}{{4}{}{A simple linear classifier}{lstlisting.4}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}A simple linear classifier}{}{lstlisting.4}}
\gincltex@bb{diagrams/generic-ML-algorithm.tex}{0}{0}{210.32863}{118.81653}
\newlabel{linearClassifierLearningAlgorithm}{{5}{}{Linear classifier learning algorithm}{lstlisting.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5}Linear classifier learning algorithm}{}{lstlisting.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A generic ML algorithm always has the above structure.\relax }}{}{figure.caption.7}}
\newlabel{fig:generic-ML-algorithm}{{3}{}{A generic ML algorithm always has the above structure.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Perceptron}{}{section.4}}
\newlabel{perceptron}{{6}{}{A perceptron implementation in MATLAB}{lstlisting.6}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6}A perceptron implementation in MATLAB}{}{lstlisting.6}}
\newlabel{perceptronTraining}{{7}{}{A perceptron learning algorithm in Java}{lstlisting.7}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {7}A perceptron learning algorithm in Java}{}{lstlisting.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Multilayer Perceptrons (MLP)}{}{subsection.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Decision trees}{}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Building a decision tree}{}{subsection.5.1}}
\newlabel{createDecisionTree}{{8}{}{An algorithm (the ID3 algorithm) to produce a decision tree in Java}{lstlisting.8}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {8}An algorithm (the ID3 algorithm) to produce a decision tree in Java}{}{lstlisting.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Entropy}{}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Overfitting decision trees}{}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The vertical line shows the cutoff point, where the error rate for testing with the validation data (red, unbroken) begins to rise, while the error rate for the training data (blue, dashed), which we're generating the decision tree from, is still improving. It is here that we should prune the tree.\relax }}{}{figure.caption.9}}
\newlabel{fig:overfitting}{{4}{}{The vertical line shows the cutoff point, where the error rate for testing with the validation data (red, unbroken) begins to rise, while the error rate for the training data (blue, dashed), which we're generating the decision tree from, is still improving. It is here that we should prune the tree.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Learning experiments in Machine Learning}{}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Cross validation}{}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Dealing with misclassifications}{}{subsection.6.2}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Ensemble Learning Algorithms}{}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Bootstrapping}{}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Bagging}{}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Boosting}{}{subsection.7.3}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Types of ML classifiers}{}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Naive Bayes Classifier}{}{section.9}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Clustering Analysis}{}{section.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Representing data}{}{subsection.10.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.1}Data matrix}{}{subsubsection.10.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.2}Distance matrix}{}{subsubsection.10.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.3}Cosine Similarity}{}{subsubsection.10.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.4}Measuring the distance of binary features}{}{subsubsection.10.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Distance for nominal features}{}{subsection.10.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Clustering approaches}{}{subsection.10.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}K-means clustering}{}{subsubsection.10.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Hierarchical Clustering}{}{subsubsection.10.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.3}Hierarchical - Agglomerative}{}{subsubsection.10.3.3}}
\gincltex@bb{diagrams/dendrogram.tex}{0}{0}{317.28181}{201.01665}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces A distance matrix for a hierarchical clustering algorithm.\relax }}{}{table.caption.12}}
\newlabel{data1}{{1}{}{A distance matrix for a hierarchical clustering algorithm.\relax }{table.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces A dendrogram produced from running the agglomerative algorithm on the data in Table\nobreakspace  {}\ref  {data1}.\relax }}{}{figure.caption.13}}
\newlabel{fig:dendrogram}{{5}{}{A dendrogram produced from running the agglomerative algorithm on the data in Table~\ref {data1}.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Cluster validation}{}{subsection.10.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.1}Internal Indexes}{}{subsubsection.10.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.2}External Indexes}{}{subsubsection.10.4.2}}
